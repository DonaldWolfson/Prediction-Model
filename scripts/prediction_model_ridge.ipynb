{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize The Prediction Model With Ridge Regression\n",
    "\n",
    "Using the data stored in `top_posts.csz.gz` this script will aim to make model that can predict the number of upvotes (likes) given all other data in the file excluding `number_of_upvotes`, `total_votes`, and `number_of_downvotes`.\n",
    "\n",
    "This script uses Ridge Regression, and tries to optmize the model's performance based on the lambda/alpha value of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import nltk\n",
    "import numpy as np\n",
    "from csv import DictReader\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dwolfson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dwolfson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a few needed packages for the nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug variables\n",
    "date = date.today().strftime(\"%b %d\")\n",
    "baseline = 0\n",
    "pred_mse = 0\n",
    "feature_list = []\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the MSE of a list of preditions & labels\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list the frequencies of a given title's Parts of Speech\n",
    "def parts_of_speech(title):\n",
    "    # Tokenize the words in the title\n",
    "    tokens = word_tokenize(title)\n",
    "    \n",
    "    # Turns each token into a pair with its value and Part of Speech label\n",
    "    # More Info Here: https://realpython.com/nltk-nlp-python/#tagging-parts-of-speech\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Map the generalized Parts of Speech to their frequency in the title\n",
    "    frequencies = {\n",
    "        \"Adjectives\":0,\n",
    "        \"Nouns\":0,\n",
    "        \"Adverbs\":0,\n",
    "        \"Pronouns\":0,\n",
    "        \"Verbs\":0,\n",
    "        \"Determiners\":0\n",
    "    }\n",
    "    \n",
    "    # Count the frequencies of each Part of Speech generalizing to 7 categories\n",
    "    for pair in pos:\n",
    "        if pair[1].startswith(\"JJ\"):\n",
    "            frequencies[\"Adjectives\"] += 1\n",
    "        elif pair[1].startswith(\"NN\"):\n",
    "            frequencies[\"Nouns\"] += 1\n",
    "        elif pair[1].startswith(\"RB\"):\n",
    "            frequencies[\"Adverbs\"] += 1\n",
    "        elif pair[1].startswith(\"PRP\"):\n",
    "            frequencies[\"Pronouns\"] += 1\n",
    "        elif pair[1].startswith(\"VB\"):\n",
    "            frequencies[\"Verbs\"] += 1\n",
    "        elif pair[1].startswith(\"DT\"):\n",
    "            frequencies[\"Determiners\"] += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return list(frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a one-hot encoding (OHE) of the hour of day, and weekday\n",
    "# OHE allows for encoding a n-length list of binary features in n - 1 space\n",
    "def one_hot_encoding_time(unixtime):\n",
    "    hour = [0] * 23\n",
    "    week = [0] * 6\n",
    "    \n",
    "    # Get the local time of the given unix timestamp\n",
    "    time = datetime.fromtimestamp(int(float(unixtime)))\n",
    "    \n",
    "    # One hot encode the hour (hour 0 is just a list of 0's)\n",
    "    # https://docs.python.org/3/library/datetime.html#datetime.datetime.hour\n",
    "    if time.hour != 0:\n",
    "        hour[time.hour - 1] = 1\n",
    "    \n",
    "    # One hot encode the weekday (day 0 is just a list of 0's)\n",
    "    # https://docs.python.org/3/library/datetime.html#datetime.date.weekday\n",
    "    if time.weekday() != 0:\n",
    "        week[time.weekday() - 1] = 1\n",
    "        \n",
    "    return hour + week\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a feature vector for a given row of data\n",
    "def feature(datum):\n",
    "    feat = [1]\n",
    "    \n",
    "    # Add a feature for the score (price of awards given)\n",
    "    feat.append(int(datum['score']))\n",
    "    \n",
    "    # Add a feature for the number of comments\n",
    "    feat.append(int(datum['number_of_comments']))\n",
    "    \n",
    "    # Add a feature for character length of title\n",
    "    # feat.append(len(datum['title']))\n",
    "    \n",
    "    # Add a feature for word length of title\n",
    "    # feat.append(len(datum['title'].strip().split(' ')))\n",
    "    \n",
    "    # Add features for the frequencies of generalized Parts of Speech\n",
    "    feat.extend(parts_of_speech(datum['title']))\n",
    "    \n",
    "    # Add features for the one-hot encoding of the Hour and Weekday\n",
    "    feat.extend(one_hot_encoding_time(datum['unixtime']))\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert debug for features\n",
    "feature_list.append('score')\n",
    "feature_list.append('number_of_comments')\n",
    "feature_list.append('parts_of_speech')\n",
    "feature_list.append('ohe_hour')\n",
    "feature_list.append('ohe_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and store each post as a list of dict elements\n",
    "with gzip.open('../data/top_posts.csv.gz', 'rt') as file:\n",
    "    csv_reader = DictReader(file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the list of all feature vectors\n",
    "X = []\n",
    "# y is the list of labels (correct values)\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in data:\n",
    "    X.append(feature(datum))\n",
    "    y.append(int(datum['number_of_upvotes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datum between training (80%), validation (10%), and test (10%)\n",
    "train = round(len(X) * 0.8)\n",
    "valid = train + round(len(X) * 0.1)\n",
    "tests = train + round(len(X) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:train]\n",
    "X_valid = X[train:valid]\n",
    "X_tests = X[valid:]\n",
    "\n",
    "y_train = y[:train]\n",
    "y_valid = y[train:valid]\n",
    "y_tests = y[valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize and fit the model to the training datas\n",
    "model = Ridge(1.0, fit_intercept=False)\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Store the MSE of the model on each lambda\n",
    "valid_perf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda: 1000\tBest MSE: 142242452.0761398\n",
      "[143563528.69676974, 143563527.453563, 143563516.2646242, 143563404.36827308, 143562284.71015686, 143551021.21061182, 143433725.32018587, 142242452.0761398]\n"
     ]
    }
   ],
   "source": [
    "for alpha in lambdas:\n",
    "    model.set_params(alpha = alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get the predictions for train and validation at lambda\n",
    "    pred_valid = model.predict(X_valid)\n",
    "\n",
    "    # Store the performance (MSE)\n",
    "    valid_perf.append(MSE(pred_valid, y_valid))\n",
    "\n",
    "# Get the best perfoming lambda for validation and run test set on it\n",
    "index = valid_perf.index(min(valid_perf))\n",
    "best_lambda = lambdas[index]\n",
    "\n",
    "print(\"Best Lambda: {}\\tBest MSE: {}\".format(best_lambda, min(valid_perf)))\n",
    "print(valid_perf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
