{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize The Prediction Model With The Top n Words\n",
    "\n",
    "Using the data stored in `top_posts.csz.gz` this script will aim to make model that can predict the number of upvotes (likes) given all other data in the file excluding `number_of_upvotes`, `total_votes`, and `number_of_downvotes`.\n",
    "\n",
    "This script aims to help further optimize the actual Prediction Model script by finding the best value `n` where `n` is the best number of popular words to include as an `n`-length binary list for the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import nltk\n",
    "import numpy as np\n",
    "from csv import writer\n",
    "from csv import DictReader\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import Ridge\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dwolfson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dwolfson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a few needed packages for the nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug variables\n",
    "date = date.today().strftime(\"%b %d\")\n",
    "baseline = 0\n",
    "pred_mse = 0\n",
    "feature_list = []\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the MSE of a list of preditions & labels\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list the frequencies of a given title's Parts of Speech\n",
    "def parts_of_speech(title):\n",
    "    # Tokenize the words in the title\n",
    "    tokens = word_tokenize(title)\n",
    "    \n",
    "    # Turns each token into a pair with its value and Part of Speech label\n",
    "    # More Info Here: https://realpython.com/nltk-nlp-python/#tagging-parts-of-speech\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Map the generalized Parts of Speech to their frequency in the title\n",
    "    frequencies = {\n",
    "        \"Adjectives\":0,\n",
    "        \"Nouns\":0,\n",
    "        \"Adverbs\":0,\n",
    "        \"Pronouns\":0,\n",
    "        \"Verbs\":0,\n",
    "        \"Determiners\":0\n",
    "    }\n",
    "    \n",
    "    # Count the frequencies of each Part of Speech generalizing to 7 categories\n",
    "    for pair in pos:\n",
    "        if pair[1].startswith(\"JJ\"):\n",
    "            frequencies[\"Adjectives\"] += 1\n",
    "        elif pair[1].startswith(\"NN\"):\n",
    "            frequencies[\"Nouns\"] += 1\n",
    "        elif pair[1].startswith(\"RB\"):\n",
    "            frequencies[\"Adverbs\"] += 1\n",
    "        elif pair[1].startswith(\"PRP\"):\n",
    "            frequencies[\"Pronouns\"] += 1\n",
    "        elif pair[1].startswith(\"VB\"):\n",
    "            frequencies[\"Verbs\"] += 1\n",
    "        elif pair[1].startswith(\"DT\"):\n",
    "            frequencies[\"Determiners\"] += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return list(frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a one-hot encoding (OHE) of the hour of day, and weekday\n",
    "# OHE allows for encoding a n-length list of binary features in n - 1 space\n",
    "def one_hot_encoding_time(unixtime):\n",
    "    hour = [0] * 23\n",
    "    week = [0] * 6\n",
    "    \n",
    "    # Get the local time of the given unix timestamp\n",
    "    time = datetime.fromtimestamp(int(float(unixtime)))\n",
    "    \n",
    "    # One hot encode the hour (hour 0 is just a list of 0's)\n",
    "    # https://docs.python.org/3/library/datetime.html#datetime.datetime.hour\n",
    "    if time.hour != 0:\n",
    "        hour[time.hour - 1] = 1\n",
    "    \n",
    "    # One hot encode the weekday (day 0 is just a list of 0's)\n",
    "    # https://docs.python.org/3/library/datetime.html#datetime.date.weekday\n",
    "    if time.weekday() != 0:\n",
    "        week[time.weekday() - 1] = 1\n",
    "        \n",
    "    return hour + week\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function returns a list that represents the presence of popular words\n",
    "def popular_words(title, n, n_popular_words):\n",
    "    words = [0] * n\n",
    "    \n",
    "    for word in word_tokenize(title):\n",
    "        if word in n_popular_words:\n",
    "            words[n_popular_words.index(word)] = 1\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a feature vector for a given row of data\n",
    "def feature(datum, n, n_popular_words):\n",
    "    feat = [1]\n",
    "    \n",
    "    # Add a feature for the score (price of awards given)\n",
    "    feat.append(int(datum['score']))\n",
    "    \n",
    "    # Add a feature for the number of comments\n",
    "    feat.append(int(datum['number_of_comments']))\n",
    "    \n",
    "    # Add a feature for character length of title\n",
    "    # feat.append(len(datum['title']))\n",
    "    \n",
    "    # Add a feature for word length of title\n",
    "    # feat.append(len(datum['title'].strip().split(' ')))\n",
    "    \n",
    "    # Add a binary feature for if the content is declared original (OC)\n",
    "    feat.append(1) if \"[oc]\" in datum['title'].lower() else feat.append(0)\n",
    "    \n",
    "    # Add features for the frequencies of generalized Parts of Speech\n",
    "    feat.extend(parts_of_speech(datum['title']))\n",
    "    \n",
    "    # Add features for the one-hot encoding of the Hour and Weekday\n",
    "    feat.extend(one_hot_encoding_time(datum['unixtime']))\n",
    "    \n",
    "    # Add feature list for the presence of any of the n-most popular words\n",
    "    feat.extend(popular_words(datum['title'], n, n_popular_words))\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert debug for features\n",
    "feature_list.append('score')\n",
    "feature_list.append('number_of_comments')\n",
    "# feature_list.append('title_length')\n",
    "feature_list.append('title_word_length')\n",
    "feature_list.append('parts_of_speech')\n",
    "feature_list.append('ohe_hour')\n",
    "feature_list.append('ohe_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "word_popularity = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and store each post as a list of dict elements\n",
    "with gzip.open('../data/top_posts.csv.gz', 'rt') as file:\n",
    "    csv_reader = DictReader(file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "        for word in word_tokenize(row['title']):\n",
    "            word_popularity[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_popularity = sorted(word_popularity.items(), key=lambda item: item[1], reverse=True)\n",
    "word_popularity = [pair[0] for pair in word_popularity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will store a list of MSE and their respective n\n",
    "n_performance = []\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:156: LinAlgWarning: Ill-conditioned matrix (rcond=3.2635e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:156: LinAlgWarning: Ill-conditioned matrix (rcond=3.2635e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:156: LinAlgWarning: Ill-conditioned matrix (rcond=3.2635e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:156: LinAlgWarning: Ill-conditioned matrix (rcond=3.2635e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:156: LinAlgWarning: Ill-conditioned matrix (rcond=3.2635e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:156: LinAlgWarning: Ill-conditioned matrix (rcond=3.2635e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "# Find the best n value\n",
    "for n in range(0, 1000, 100):\n",
    "    # Debug to know how far the script is\n",
    "    print(n)  \n",
    "      \n",
    "    # X is the list of all feature vectors\n",
    "    X = []\n",
    "    # y is the list of labels (correct values)\n",
    "    y = []\n",
    "    # List of the n-most popular words\n",
    "    n_popular_words = word_popularity[:n]\n",
    "    \n",
    "    for datum in data:\n",
    "        X.append(feature(datum, n, n_popular_words))\n",
    "        y.append(int(datum['number_of_upvotes']))\n",
    "    \n",
    "    # Split the datum between training (80%), validation (10%), and test (10%)\n",
    "    train = round(len(X) * 0.8)\n",
    "    valid = train + round(len(X) * 0.1)\n",
    "    tests = train + round(len(X) * 0.1)\n",
    "    \n",
    "    X_train = X[:train]\n",
    "    X_valid = X[train:valid]\n",
    "    X_tests = X[valid:]\n",
    "\n",
    "    y_train = y[:train]\n",
    "    y_valid = y[train:valid]\n",
    "    y_tests = y[valid:]\n",
    "    \n",
    "    # Intialize and fit the model to the training datas\n",
    "    model = Ridge(1.0, fit_intercept=False)\n",
    "    \n",
    "    # Try different alpha values for Ridge Regression\n",
    "    for alpha in lambdas:\n",
    "        model.set_params(alpha=alpha)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Test model on validation\n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        n_performance.append((MSE(y_valid_pred, y_valid), n, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Pair: (139729425.21009746, 600, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Report the best MSE and n\n",
    "best_pair = min(n_performance, key = lambda x: x[0])\n",
    "print(\"Best Pair: {}\".format(best_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(143228630.98354718, 0, 0),\n",
       " (143228629.84047648, 0, 0.001),\n",
       " (143228619.5527776, 0, 0.01),\n",
       " (143228516.66854152, 0, 0.1),\n",
       " (143227487.10482565, 0, 1),\n",
       " (143217121.90283233, 0, 10),\n",
       " (143108560.37919426, 0, 100),\n",
       " (141985044.62956142, 0, 1000),\n",
       " (146090511.63023436, 100, 0),\n",
       " (146090507.0913622, 100, 0.001),\n",
       " (146090466.24171868, 100, 0.01),\n",
       " (146090057.76441035, 100, 0.1),\n",
       " (146085974.83773828, 100, 1),\n",
       " (146045302.2085573, 100, 10),\n",
       " (145650723.2929844, 100, 100),\n",
       " (142424668.58364403, 100, 1000),\n",
       " (145685150.53921163, 200, 0),\n",
       " (145685145.25373057, 200, 0.001),\n",
       " (145685097.68829527, 200, 0.01),\n",
       " (145684622.4130433, 200, 0.1),\n",
       " (145679902.66864437, 200, 1),\n",
       " (145634110.91463798, 200, 10),\n",
       " (145202285.2877293, 200, 100),\n",
       " (141914154.87885594, 200, 1000),\n",
       " (145679643.74490508, 300, 0),\n",
       " (145679635.17767608, 300, 0.001),\n",
       " (145679558.07939652, 300, 0.01),\n",
       " (145678787.76111695, 300, 0.1),\n",
       " (145671144.31719187, 300, 1),\n",
       " (145598040.69334373, 300, 10),\n",
       " (144989557.600222, 300, 100),\n",
       " (141488896.46632728, 300, 1000),\n",
       " (144990750.9515122, 400, 0),\n",
       " (144990741.28180715, 400, 0.001),\n",
       " (144990654.26301572, 400, 0.01),\n",
       " (144989784.91070226, 400, 0.1),\n",
       " (144981166.29001153, 400, 1),\n",
       " (144899087.63716742, 400, 10),\n",
       " (144230218.43762612, 400, 100),\n",
       " (140813441.49824873, 400, 1000),\n",
       " (151566111.7836268, 500, 0),\n",
       " (151564632.1400996, 500, 0.001),\n",
       " (151551344.66732773, 500, 0.01),\n",
       " (151421313.14936566, 500, 0.1),\n",
       " (150355953.47579625, 500, 1),\n",
       " (147090067.1636329, 500, 10),\n",
       " (144859346.08817333, 500, 100),\n",
       " (140829618.4242166, 500, 1000),\n",
       " (150627007.90229377, 600, 0),\n",
       " (150625454.91236007, 600, 0.001),\n",
       " (150611508.84078878, 600, 0.01),\n",
       " (150475038.45022193, 600, 0.1),\n",
       " (149357316.17671767, 600, 1),\n",
       " (145925141.24998567, 600, 10),\n",
       " (143428026.1837105, 600, 100),\n",
       " (139729425.21009746, 600, 1000),\n",
       " (6.028125513065109e+35, 700, 0),\n",
       " (149093565.66347912, 700, 0.001),\n",
       " (149082747.14023504, 700, 0.01),\n",
       " (148978106.66849956, 700, 0.1),\n",
       " (148190131.62930465, 700, 1),\n",
       " (146157764.34767422, 700, 10),\n",
       " (144164467.227046, 700, 100),\n",
       " (139947416.7091203, 700, 1000),\n",
       " (150544250.79022008, 800, 0),\n",
       " (150542976.50998944, 800, 0.001),\n",
       " (150531549.53180966, 800, 0.01),\n",
       " (150421236.31181234, 800, 0.1),\n",
       " (149602896.6350048, 800, 1),\n",
       " (147559225.84852305, 800, 10),\n",
       " (145252945.64235726, 800, 100),\n",
       " (140225623.47769344, 800, 1000),\n",
       " (5.169895339516446e+37, 900, 0),\n",
       " (151923690.6642278, 900, 0.001),\n",
       " (151910792.09490296, 900, 0.01),\n",
       " (151786703.22285157, 900, 0.1),\n",
       " (150891523.2356141, 900, 1),\n",
       " (148815693.10052356, 900, 10),\n",
       " (146156153.40394115, 900, 100),\n",
       " (140386588.8014766, 900, 1000)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_performance"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
