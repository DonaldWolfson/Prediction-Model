{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abltion Analysis of The Prediction Model\n",
    "\n",
    "Using the data stored in `top_posts.csz.gz` this script will aim to make model that can predict the number of upvotes (likes) given all other data in the file excluding `number_of_upvotes`, `total_votes`, and `number_of_downvotes`.\n",
    "\n",
    "This script attempts to get a better understanding of how each feature influences the model through [Ablation Analysis](https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it). This means the model will be ran with specific features removed to compare their performance on the same dataset. To reduce clutter, this script uses methods stored in `ablation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from ablation import *\n",
    "from csv import writer\n",
    "from csv import DictReader\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import Ridge\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dwolfson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dwolfson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a few needed packages for the nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug variables\n",
    "date = date.today().strftime(\"%b %d\")\n",
    "baseline = 0\n",
    "pred_mse = 0\n",
    "feature_list = []\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most optimal size found for n-length list of popular words\n",
    "n = 600\n",
    "\n",
    "# Most optimal alpha found for Ridge Regression\n",
    "alpha = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "word_popularity = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and store each post as a list of dict elements\n",
    "with gzip.open('../data/top_posts.csv.gz', 'rt') as file:\n",
    "    csv_reader = DictReader(file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "        for word in word_tokenize(row['title']):\n",
    "            word_popularity[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_popularity = sorted(word_popularity.items(), key=lambda item: item[1], reverse=True)\n",
    "word_popularity = [pair[0] for pair in word_popularity]\n",
    "word_popularity = word_popularity[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data since it's sorted by subreddit to give a fairer distribution\n",
    "random.shuffle(data)\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store an X vector for each feature vector\n",
    "X_all = []\n",
    "X_exc_score = []\n",
    "X_exc_num_com = []\n",
    "X_exc_len_char = []\n",
    "X_exc_len_word = []\n",
    "X_exc_oc = []\n",
    "X_exc_pos = []\n",
    "X_exc_ohe = []\n",
    "X_exc_popular_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the list of labels (correct values)\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in data:\n",
    "    X_all.append(feature_all(datum, word_popularity, n))\n",
    "    X_exc_score.append(feature_exc_score(datum, word_popularity, n))\n",
    "    X_exc_num_com.append(feature_exc_num_com(datum, word_popularity, n))\n",
    "    X_exc_len_char.append(feature_exc_len_char(datum, word_popularity, n))\n",
    "    X_exc_len_word.append(feature_exc_len_word(datum, word_popularity, n))\n",
    "    X_exc_oc.append(feature_exc_oc(datum, word_popularity, n))\n",
    "    X_exc_pos.append(feature_exc_pos(datum, word_popularity, n))\n",
    "    X_exc_ohe.append(feature_exc_ohe(datum, word_popularity, n))\n",
    "    X_exc_popular_word.append(feature_exc_popular_word(datum, word_popularity, n))\n",
    "    y.append(int(datum['number_of_upvotes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House Cleaning\n",
    "del data\n",
    "del popular_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datum between training (80%), validation (10%), and test (10%)\n",
    "train = round(len(X_all) * 0.8)\n",
    "valid = train + round(len(X_all) * 0.1)\n",
    "tests = train + round(len(X_all) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:train]\n",
    "y_valid = y[train:valid]\n",
    "y_tests = y[valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a baseline by testing the model against the average label\n",
    "average_likes = np.mean(y)\n",
    "y_avg = [average_likes] * len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MSE from the baseline averages\n",
    "baseline = MSE(y_avg[train:valid], y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_all[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_all[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_length|title_word_length|orginal_content|parts_of_speech|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_score[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_score[train:valid]), y_valid))\n",
    "row.append('number_of_comments|title_length|title_word_length|orginal_content|parts_of_speech|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_num_com[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_num_com[train:valid]), y_valid))\n",
    "row.append('score|title_length|title_word_length|orginal_content|parts_of_speech|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_num_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_len_char[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_len_char[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_word_length|orginal_content|parts_of_speech|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_len_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_len_word[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_len_word[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_length|orginal_content|parts_of_speech|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_len_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_oc[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_oc[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_length|title_word_length|parts_of_speech|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_pos[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_pos[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_length|title_word_length|orginal_content|ohe_hour|ohe_week|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_ohe[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_ohe[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_length|title_word_length|orginal_content|parts_of_speech|{}_popular_words'.format(n))\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = Ridge(fit_intercept=False, alpha=alpha)\n",
    "model.fit(X_exc_popular_word[:train], y_train)\n",
    "\n",
    "# Store the performance of this model\n",
    "row = []\n",
    "row.append(date)\n",
    "row.append(baseline)\n",
    "row.append(MSE(model.predict(X_exc_popular_word[train:valid]), y_valid))\n",
    "row.append('score|number_of_comments|title_length|title_word_length|orginal_content|parts_of_speech|ohe_hour|ohe_week')\n",
    "row.append(shuffle)\n",
    "\n",
    "performance.append(row)\n",
    "\n",
    "del X_exc_popular_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the debug data from this script to the CSV of MSE records\n",
    "with open('../data/prediction_model_MSE.csv', 'a') as file:\n",
    "    csv_writer = writer(file)\n",
    "    \n",
    "    for row in performance:\n",
    "        csv_writer.writerow(row) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
